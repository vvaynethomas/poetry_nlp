{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "482594c7-a937-42e3-bee4-b458380c8a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sys\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dcde1e-3c97-473a-b996-07d947d4fedf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b08a40b-5db2-4b06-983e-ab735ae06694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('poetry_corpus.pkl', 'rb') as f:\n",
    "    corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b60c455-bccb-4692-b8fb-832e0e0f3ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "poems_in_lines = [poem.splitlines() for poem in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25506942-1f90-4ee2-b485-42b889b8010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_in_lines = [line for poem in poems_in_lines for line in poem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7735bf6c-2131-44d9-8de0-34448ecd15f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear Writers, I’m compiling the first in what I hope is a series of publications I’m calling artists among artists. The theme for issue 1 is “Faggot Dinosaur.” I hope to hear from you! Thank you and best wishes.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_in_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a5d3e8-d42d-4aee-9008-25ceeab1c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re\n",
    "from nltk.corpus import words\n",
    "import multiprocessing as mp\n",
    "# def clean_text(text):\n",
    "#     if len(sent_tokenize(text)) > 1:\n",
    "#         return ''\n",
    "#     text = text.lower()\n",
    "#     text = text.replace('\\%','')\n",
    "#     text = re.sub('\\[.*?\\]', '', text)\n",
    "#     text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "#     text = re.sub('<.*?>+', '', text)\n",
    "#     text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "#     text = re.sub('\\n', ' ', text)\n",
    "#     text = re.sub('\\w*\\d\\w*', '', text)\n",
    "#     text = \" \".join(filter(lambda x:x[0]!=\"@\", text.split()))\n",
    "#     return text\n",
    "\n",
    "def clean_text(text):\n",
    "    cont_dict = { \n",
    "        \"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "        \"aren't\": \"are not / am not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"can't've\": \"cannot have\",\n",
    "        \"'cause\": \"because\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"couldn't've\": \"could not have\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"hadn't've\": \"had not have\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he had / he would\",\n",
    "        \"he'd've\": \"he would have\",\n",
    "        \"he'll\": \"he shall / he will\",\n",
    "        \"he'll've\": \"he shall have / he will have\",\n",
    "        \"he's\": \"he has / he is\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"how'd'y\": \"how do you\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how's\": \"how has / how is / how does\",\n",
    "        \"I'd\": \"I had / I would\",\n",
    "        \"I'd've\": \"I would have\",\n",
    "        \"I'll\": \"I shall / I will\",\n",
    "        \"I'll've\": \"I shall have / I will have\",\n",
    "        \"I'm\": \"I am\",\n",
    "        \"I've\": \"I have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it had / it would\",\n",
    "        \"it'd've\": \"it would have\",\n",
    "        \"it'll\": \"it shall / it will\",\n",
    "        \"it'll've\": \"it shall have / it will have\",\n",
    "        \"it's\": \"it has / it is\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"ma'am\": \"madam\",\n",
    "        \"mayn't\": \"may not\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mightn't've\": \"might not have\",\n",
    "        \"must've\": \"must have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"mustn't've\": \"must not have\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"needn't've\": \"need not have\",\n",
    "        \"o'clock\": \"of the clock\",\n",
    "        \"oughtn't\": \"ought not\",\n",
    "        \"oughtn't've\": \"ought not have\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"sha'n't\": \"shall not\",\n",
    "        \"shan't've\": \"shall not have\",\n",
    "        \"she'd\": \"she had / she would\",\n",
    "        \"she'd've\": \"she would have\",\n",
    "        \"she'll\": \"she shall / she will\",\n",
    "        \"she'll've\": \"she shall have / she will have\",\n",
    "        \"she's\": \"she has / she is\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"shouldn't've\": \"should not have\",\n",
    "        \"so've\": \"so have\",\n",
    "        \"so's\": \"so as / so is\",\n",
    "        \"that'd\": \"that would / that had\",\n",
    "        \"that'd've\": \"that would have\",\n",
    "        \"that's\": \"that has / that is\",\n",
    "        \"there'd\": \"there had / there would\",\n",
    "        \"there'd've\": \"there would have\",\n",
    "        \"there's\": \"there has / there is\",\n",
    "        \"they'd\": \"they had / they would\",\n",
    "        \"they'd've\": \"they would have\",\n",
    "        \"they'll\": \"they shall / they will\",\n",
    "        \"they'll've\": \"they shall have / they will have\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"to've\": \"to have\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we had / we would\",\n",
    "        \"we'd've\": \"we would have\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"we'll've\": \"we will have\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"what'll\": \"what shall / what will\",\n",
    "        \"what'll've\": \"what shall have / what will have\",\n",
    "        \"what're\": \"what are\",\n",
    "        \"what's\": \"what has / what is\",\n",
    "        \"what've\": \"what have\",\n",
    "        \"when's\": \"when has / when is\",\n",
    "        \"when've\": \"when have\",\n",
    "        \"where'd\": \"where did\",\n",
    "        \"where's\": \"where has / where is\",\n",
    "        \"where've\": \"where have\",\n",
    "        \"who'll\": \"who shall / who will\",\n",
    "        \"who'll've\": \"who shall have / who will have\",\n",
    "        \"who's\": \"who has / who is\",\n",
    "        \"who've\": \"who have\",\n",
    "        \"why's\": \"why has / why is\",\n",
    "        \"why've\": \"why have\",\n",
    "        \"will've\": \"will have\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"won't've\": \"will not have\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"wouldn't've\": \"would not have\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"y'all'd\": \"you all would\",\n",
    "        \"y'all'd've\": \"you all would have\",\n",
    "        \"y'all're\": \"you all are\",\n",
    "        \"y'all've\": \"you all have\",\n",
    "        \"you'd\": \"you had / you would\",\n",
    "        \"you'd've\": \"you would have\",\n",
    "        \"you'll\": \"you shall / you will\",\n",
    "        \"you'll've\": \"you shall have / you will have\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\"\n",
    "        }\n",
    "\n",
    "    stopwords_list = stopwords.words(\"english\")\n",
    "    try:\n",
    "        while text.startswith('b'):\n",
    "            text = text[1:]\n",
    "        text = text.lower()\n",
    "        for word in cont_dict.keys():\n",
    "            pattern = re.compile(f'({str(word)})',re.I)\n",
    "            text = pattern.sub(cont_dict[word],text)\n",
    "        tokens = word_tokenize(text)\n",
    "        text = ' '.join([word for word in tokens if word not in stopwords_list])\n",
    "        text = ' '.join([''.join([c if c.isalnum() else ' ' for c in word]) for word in word_tokenize(text)])\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        text = text.replace('\\%','')\n",
    "        text = re.sub('\\[.*?\\]', '', text)\n",
    "        text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "        text = re.sub('<.*?>+', '', text)\n",
    "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "        text = re.sub('\\n', ' ', text)\n",
    "        text = re.sub('\\w*\\d\\w*', '', text)\n",
    "        text = \" \".join(filter(lambda x:x[0]!=\"@\", text.split()))\n",
    "        return text.strip()\n",
    "    \n",
    "    except AttributeError:\n",
    "        text = str(text)\n",
    "        return preprocess_text(text)\n",
    "    \n",
    "    except TypeError:\n",
    "        text = text.decode()[1:]\n",
    "        return preprocess(text.strip())\n",
    "\n",
    "with mp.Pool(processes=mp.cpu_count()-1) as pool:\n",
    "    clean_corpus = pool.map(clean_text, corpus_in_lines)\n",
    "# p = Process(target=clean_text, args=)\n",
    "# clean_corpus = Parallel()(delayed(clean_text)(line) for line in corpus_in_lines)\n",
    "# clean_corpus = list(map(clean_text, corpus_in_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d62c4-6aff-4a8a-b3e8-51791022ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('clean_corpus.pkl', 'wb') as f:\n",
    "    pickle.dump(clean_corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a87f9-c0f7-466d-9a9a-b542b2aabb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus_lines = [line for line in clean_corpus if len(word_tokenize(line)) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10e4cf02-6fd3-4811-b32d-95fb4bc0ccbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i think that i shall never fear\n"
     ]
    }
   ],
   "source": [
    "print(clean_corpus_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8885f52b-d176-4364-8a56-5c4e6df9c1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139400\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5)\n",
    "tokenizer.fit_on_texts(clean_corpus_lines)\n",
    "# input_sequences = tokenizer.texts_to_sequences(corpus_lines)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(total_words)\n",
    "input_sequences = []\n",
    "for line in corpus_lines:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4293ca3d-d842-451f-8bfa-65d58212b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "word_df = pd.DataFrame.from_dict(tokenizer.word_index, orient='index', columns=['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0909f69a-9af2-4d8c-9060-fc3d04e0fa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     count\n",
      "the      1\n",
      "and      2\n",
      "of       3\n",
      "a        4\n",
      "to       5\n",
      "                 count\n",
      "protoplanetary  139395\n",
      "moogoogaipan    139396\n",
      "nottooold       139397\n",
      "gooing          139398\n",
      "morninga        139399\n"
     ]
    }
   ],
   "source": [
    "print(word_df.head())\n",
    "print(word_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f301c-ec7b-4fc5-a65f-8a9691f308fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
